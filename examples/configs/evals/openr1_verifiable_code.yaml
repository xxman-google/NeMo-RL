# OpenR1 verifiable coding evaluation Configuration
defaults: "eval.yaml"

data:
  prompt_file: "examples/prompts/openr1_verifiable_code.txt"
  dataset_name: "openr1_verifiable_code"
  code_exe_dir: "/tmp/logs"
  source: "code_contests"

generation:
  temperature: 0.7
  top_p: 0.8
  model_name: "Qwen/Qwen3-8B"
  vllm_cfg:
    max_model_len: 8192

env:
  env_type: "code"
  code:
    num_workers: 8
    terminate_on_evaluation: true
    worker_type: "python_stdout_verify"
    timeout: 3.0
